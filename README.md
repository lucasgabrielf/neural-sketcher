## Neural Sketcher is an interactive game where the user plays against a neural network model

In the game, the user is prompted to draw an element of the everyday-life, and if, at the end, the AI model successfully classifies it, the user wins the round, whereas if it gets wrong, the user loses a life-point. At the end of the game, all drawings generated by the user will be used to train the model classifier.

## Playing neural sketcher:
<p align="center">
  <img width="600" src="https://github.com/user-attachments/assets/d0119f05-3cfe-4d61-82d1-fa87fab6ad4a" alt="Neural Sketcher Gif" />
</p>

<div style="width: 100vw; display: flex; justify-content: center; flex-direction: row; align-items: center;">
  <img width="333" alt="image" src="https://github.com/user-attachments/assets/776dd26d-902e-4818-b475-9705431a7a04" />
  <img width="333" alt="image" src="https://github.com/user-attachments/assets/05ed92cd-bc36-44c8-8034-b68fb7bd1e59" />
  <img width="333" alt="image" src="https://github.com/user-attachments/assets/15d88d26-f1f1-4d7b-b32c-51c04e39d9dd" />
</div>



# Technical details
Neural Sketcher was created by using the Single Page Only (SPA) architecture, allowing for the complete segregation of the backend and frontend by using one server for each.
* **Frontend:** Created with **React.js**, it is the cliend-sided server that loads and renders the key components of the ui, with **TSX** beeing extensivelly used in api calls to the backend endpoints. It has the following responsabilities:
  * Capturing the user's drawing on an HTML canvas.
  * Sending the image data to the backend API for processing.
  * Receiving the prediction results and rendering a dynamic data visualization of the model's confidence scores.
* **Backend:** Created with **Python** and **FastAPI** It exposes an endpoint that, upon receiving an image from the client, performs the following pipeline:
  * Image Standardization: The input image is compressed and converted to the MNIST dataset format (a 28x28 pixel grayscale image).
  * Data Preprocessing: The image is transformed into a NumPy array, its pixel values are normalized to a [0, 1] range, and it is reshaped to match the model's expected input dimensions (1, 28, 28, 1).
  * Inference: The preprocessed image is fed into the pre-loaded TensorFlow model to generate predictions.
  * Response: The API returns a JSON object containing the model's confidence scores (a softmax probability distribution) for each digit (0-9).
For efficiency, the trained model is loaded into memory only once when the backend server initializes, ensuring low latency for all prediction requests.
* **Neural Network Model:** Created with **TensorFlow/Keras**, it is composed of two main parts:
  * Feature Extractor: two **CNN/Batch Nomalization/MaxPooling** layers with relu activation. These layers are responsible for identifying key patterns and features (like edges, corners, and textures) in the drawing.
  * Classifier: A **Dense Layer** with **Droupot** and relu activation, with a final **Dense Layer** output with a 10-class softmax activation.
* **Data Visualization:** At each call to the POST endpoint in the backend that saves the image and feeds it to the model, the backend does the steps outlined above, in order, returning the softmax probability distributions predicted by the model in the possible classes (digits from 0 to 9). Those values, represented with a python list, is then passed in the response body, saved into **CSS Custom Property** of each **ProbCircle.tsx** element and used to render an animation of the probability distribution.
